# -*- coding: utf-8 -*-
"""(Default prediction).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rmvce4JEjg7H581jG1avw1lCXFhSuMvM

Default Prediction in the loan process 
first of all we import all necessary libraries for the data cleaning and interpretation process
"""

# Commented out IPython magic to ensure Python compatibility.
#import all libraries 
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
import matplotlib.style as style
import itertools
import seaborn as sns 
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

train = pd.read_csv("/content/drive/MyDrive/dataset default/train.csv")

"""DATA INTERPRETATION """

train.head()

train.info()

train.describe()

train.isnull().sum()

train.info(verbose=True)

train

train.head()

train.isnull().sum()

test = pd.read_csv("/content/drive/MyDrive/dataset default/test.csv")

test.head()

test.describe()

test.info()

test.isnull().sum()

train.value_counts()

"""EXPLORATORY DATA ANALYSIS"""

#univariate distribution
plt.figure(figsize=(6,6))
sns.kdeplot(train['target'])
plt.show()

plt.figure(figsize=(6,6))
sns.kdeplot(test['var_9'])
plt.show()

#there is single point distribution found in test data because there are null values are in dataset 
#so thst we impute these null values by meadian 
test['var_9'].fillna(test['var_9'].median(),inplace = True)

#Impute random column with mode as the distribution is closely similar:
plt.figure(figsize=(6,6))
sns.kdeplot(test['var_10'][pd.notnull(test['var_10'])])
plt.show()

"""DATA PRE-PROCESSING"""

new_train = train.loc[:, 'target']

new_test = test.loc[:, 'id']

New_test = pd.DataFrame({'id':new_test.index, 'id':new_test.values})

New_test.to_csv("New_test.csv")

New_train = pd.DataFrame({'ID':new_train.index, 'Target':new_train.values})

New_train.head()

New_train.to_csv("New_train.csv")

train = train.drop('id',axis=1)

train = train.drop("target",axis=1)

train.columns

probability = train.iloc[1:25001].mean(axis=1)

probability.to_csv("Probability_predicted.csv")

test = test.drop('id', axis=1)

test

Test = test.iloc[1:5001].mean(axis=1)

Test.head()

Test1 = Test.replace([np.inf, -np.inf], 0)

Test1

Test1.to_csv("Test_data.csv")

new_test = pd.read_csv("/content/drive/MyDrive/New_test.csv")
Test_data = pd.read_csv("/content/drive/MyDrive/Test_data.csv")

new_test

New_test = pd.DataFrame({'ID':new_train.index, 'id':new_train.values})

New_test

Test_data.head()

Test_data = pd.DataFrame({'ID':new_train.index, 'Predicted_probability':new_train.values})

Test_data.head()

New_Test_Data = pd.merge(New_test,Test_data[["ID","Predicted_probability"]], on="ID", how='left')

New_Test_Data.head()

probability1 = train.iloc[1:5001].mean(axis=1)

probability.to_csv("Probability_Predicted1.csv")

Probability_mod = probability.replace([np.inf, -np.inf], 0)

Probability_mod

Probability_mod1 = probability1.replace([np.inf,-np.inf], 0)

Probability_mod1

Probability_mod.to_csv("Probability_pred.csv")

Probability_mod1.to_csv("Probability_pred1.csv")

"""**now we download all the modifiled file after converting into csv **"""

Train  = pd.read_csv("/content/drive/MyDrive/Probability_pred.csv")

Test = pd.read_csv("/content/drive/MyDrive/Probability_pred1.csv")

Train.head()

Test.head()

New_train = pd.read_csv("/content/drive/MyDrive/New_train.csv")

New_train.head()

New_Train = pd.DataFrame({'ID':new_train.index, 'Target':new_train.values})

New_Train1 = pd.merge(New_train,Train[['ID','Predicted_probability']],on='ID', how='left')

New_Train1.fillna(0)

New_Train1.info()

New_Train1.to_csv("New_Train1.csv")

Df = pd.read_csv("New_Train1.csv")

Df.head()

Df.fillna(0)

Df.to_csv("Training_data.csv")

Training_data = pd.read_csv("/content/drive/MyDrive/Training_data.csv")

Training_data.head()

"""MODEL EVALUATION """

X = Training_data["Target"].values
y = Training_data["Predicted_probability"].values

X = X.reshape(-1,1)
y = y.reshape(-1,1)

from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.33)

from sklearn.preprocessing import StandardScaler
SC = StandardScaler()
X_train = SC.fit_transform(X_train)
y_train = SC.fit_transform(y_train)

from sklearn.ensemble import GradientBoostingRegressor
reg = GradientBoostingRegressor(learning_rate=0.01, max_depth=4, min_samples_split=5,n_estimators=500)
reg.fit(X_train, y_train)

X_test

y_pred = reg.predict(X_test)

y_pred

from sklearn.metrics import mean_squared_error
mean_squared_error(y_test, y_pred, squared=False)

"""PREDICTED MODEL On TEST DATA SET """

#now we have test data to
New_Test_Data.head()

pd.DataFrame(New_Test_Data).to_csv("Final_results.csv")

X1 = New_Test_Data["id"].values
y1 = New_Test_Data["Predicted_probability"].values

X1 = X.reshape(-1,1)
y1= y.reshape(-1,1)

from sklearn.model_selection import train_test_split
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, random_state=0, test_size=0.33)

X1_test

pd.DataFrame(X1_test).to_csv("predicted.csv")

y1_pred = reg.predict(X1_test)

y1_pred

pd.DataFrame(y1_pred).to_csv("probability.csv")

import pickle
pickle.dump(reg, open('model.pkl', 'wb'))

import pickle
from flask import Flask, request, jsonify, render_template

# load model
model = pickle.load(open('Defaultprediction.pickle','rb'))

model = pickle.load(open('model.pkl','rb'))
print(model.predict([[1]]))

pip install streamlit

"""YOU CAN BUILD STREAM LIT APP """

import streamlit as st 
prediction = st.sidebar.slider('Predicted_probability', 0.014019556, -0.081322535)

"""FLASK API"""

#Import libraries
import numpy as np
from flask import Flask, request, jsonify
import pickle
import joblib

app = Flask(__name__)

# Load the model
model = joblib.load(open('Defaultprediction.pickle','rb'))

@app.route('/')
def home():
    return 'See this apps documentation on this <a href="https://github.com/arvind2u">github link</a>'
@app.route('/api',methods=['POST'])
def predict():
    # Get the data from the POST request.
    datas = request.get_json(force=True)

    # Make prediction using model loaded from disk
    pred=[]
    for data in datas:
        prediction = model.predict([np.array([data['LIMIT_BAL'], data['PAY_1'], data['BILL_AMT1']])])

        # Take the first value of prediction
        output = int(prediction[0])
        out = "Defaulter" if output == 1 else "Repayer"
        pred.append(out)
        
    return jsonify(pred)

''' After analysing the datasets, there are few attributes of a client with which the bank would be able to 
identify if they will repay the loan or not.
main attributes from which bank can analyse the whole process that
1. NAME_HOUSING_TYPE
2. AMT_INCOME
3. AMT_CREDIT
4. CNT_CHILDREN AND CNT_FAM_MEMBERS
5.NAME_CASH_LOAN_PURPOSEB 
There are reasons behind rejecting the loan process by bank to many customers. 
 1. Either they are paying lately. Or they are paying with due amount. 
 2. Or they are not fit for  the terms and conditions of the bank. '''

